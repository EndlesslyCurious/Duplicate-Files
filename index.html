<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0080)http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/ -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" itemscope="" itemtype="http://schema.org/Article" prefix="og: http://ogp.me/ns#"><head profile="http://gmpg.org/xfn/11"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Finding duplicate files using Python | Endlessly Curious</title>    
    <link rel="stylesheet" href="./Finding duplicate files using Python   Endlessly Curious_files/style.css" type="text/css" media="screen">
	<link rel="stylesheet" href="./Finding duplicate files using Python   Endlessly Curious_files/superfish.css" type="text/css" media="screen">
    <!--[if IE 6]><link rel="stylesheet" href="http://www.endlesslycurious.com/wp-content/themes/min/ie6.css" type="text/css" media="screen" /><![endif]-->
	<!-- Additional CSS add-ons from custom admin page --><style type="text/css" media="screen">#main { background:none; border:0; padding-top:10px; }</style>	

	<link rel="pingback" href="http://www.endlesslycurious.com/xmlrpc.php">
	<link rel="alternate" type="application/rss+xml" title="Endlessly Curious RSS Feed" href="http://www.endlesslycurious.com/feed/">
	<link rel="shortcut icon" href="http://www.endlesslycurious.com/wp-content/favicon.png">

    
<!-- All in One SEO Pack 2.2.6 by Michael Torbert of Semper Fi Web Designob_start_detected [-1,-1] -->
<meta name="keywords" itemprop="keywords" content="python">

<link rel="canonical" href="./Finding duplicate files using Python   Endlessly Curious_files/Finding duplicate files using Python   Endlessly Curious.html">
<meta property="og:title" content="Finding duplicate files using Python | Endlessly Curious">
<meta property="og:type" content="article">
<meta property="og:url" content="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/">
<meta property="og:site_name" content="Endlessly Curious">
<meta property="og:description" content="I wrote this script to find and optionally delete duplicate files in a directory tree.  The script uses MD5 hashes of each file&#39;s content to detect duplicate files. This script is based on zalew&#39;s answer on stackoverflow. So far I have found this script sufficient for accurately finding and removing duplicate files in my photograph collection.  &quot;&quot;&quot;Find duplicate files inside a directory tree.&quot;&quot;&quot; from os import walk, remove, stat from os.path import join as joinpath from md5 import md5 def find_duplicates( rootdir ): &quot;&quot;&quot;Find duplicate files in directory tree.&quot;&quot;&quot; filesizes = {} # Build up dict with key as filesize and value is list of filenames. for path, dirs, files in walk( rootdir ): for filename in files: filepath = joinpath( path, filename ) filesize = stat( filepath ).st_size filesizes.setdefault( filesize,  # We are only interested in lists with more than one entry. for files in : for filepath in files: with open( filepath ) as openfile: filehash = md5( openfile.read()">
<meta property="article:publisher" content="https://www.facebook.com/EndlesslyCurious">
<meta property="article:published_time" content="2011-06-01T01:00:32Z">
<meta property="article:modified_time" content="2013-12-29T11:50:15Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@dpbrown">
<meta name="twitter:domain" content="www.EndlesslyCurious.com">
<meta name="twitter:description" content="I wrote this script to find and optionally delete duplicate files in a directory tree.  The script uses MD5 hashes of each file&#39;s content to detect duplicate files. This script is based on zalew&#39;s answer on stackoverflow. So far I have found this script sufficient for accurately finding and removing duplicate files in my photograph collection.  &quot;&quot;&quot;Find duplicate files inside a directory tree.&quot;&quot;&quot; from os import walk, remove, stat from os.path import join as joinpath from md5 import md5 def find_duplicates( rootdir ): &quot;&quot;&quot;Find duplicate files in directory tree.&quot;&quot;&quot; filesizes = {} # Build up dict with key as filesize and value is list of filenames. for path, dirs, files in walk( rootdir ): for filename in files: filepath = joinpath( path, filename ) filesize = stat( filepath ).st_size filesizes.setdefault( filesize,  # We are only interested in lists with more than one entry. for files in : for filepath in files: with open( filepath ) as openfile: filehash = md5( openfile.read()">
		<script id="twitter-wjs" src="./Finding duplicate files using Python   Endlessly Curious_files/widgets.js"></script><script type="text/javascript" async="" src="./Finding duplicate files using Python   Endlessly Curious_files/ga.js"></script><script type="text/javascript">
		  var _gaq = _gaq || [];
		  _gaq.push(['_setAccount', 'UA-5694100-1']);
		  _gaq.push(['_trackPageview']);
		  (function() {
		    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		  })();
		</script>
<!-- /all in one seo pack -->
<link rel="alternate" type="application/rss+xml" title="Endlessly Curious » Finding duplicate files using Python Comments Feed" href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/feed/">
<link rel="stylesheet" id="cptch_stylesheet-css" href="./Finding duplicate files using Python   Endlessly Curious_files/style(1).css" type="text/css" media="all">
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/jquery.js"></script><script>jQueryWP = jQuery;</script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/jquery-migrate.min.js"></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.endlesslycurious.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.endlesslycurious.com/wp-includes/wlwmanifest.xml"> 
<link rel="prev" title="Garr – Lessons from Bamboo" href="http://www.endlesslycurious.com/2011/05/27/garr-lessons-from-bamboo/">
<link rel="next" title="Python 2.7.1 Goodness" href="http://www.endlesslycurious.com/2011/06/10/python-2-7-1-goodness/">
<meta name="generator" content="WordPress 4.1.1">
<link rel="shortlink" href="http://wp.me/pn1OI-Hd">
<link rel="stylesheet" type="text/css" href="./Finding duplicate files using Python   Endlessly Curious_files/shCore.css"><link rel="stylesheet" type="text/css" href="./Finding duplicate files using Python   Endlessly Curious_files/shThemeDefault.css"><style type="text/css" id="syntaxhighlighteranchor"></style>
</head>
<body class="single single-post postid-2679 single-format-standard" data-twttr-rendered="true">


<div id="top">

	<div id="header" class="clearfix">
        <div class="contentwidth">
            <div id="logo" class="left"><p><a href="http://www.endlesslycurious.com/"><img src="./Finding duplicate files using Python   Endlessly Curious_files/logo-min-2012.jpg" alt="Endlessly Curious"></a> Scottish programmer and photographer exploring the world.</p></div>
            <div id="menu" class="right">
            
            	<ul class="sf-menu sf-js-enabled sf-shadow">
					<li><a href="http://www.endlesslycurious.com/" title="Home">Home</a></li>
                	<li class="page_item page-item-2"><a href="http://www.endlesslycurious.com/about/">About</a></li>
<li class="page_item page-item-3508"><a href="http://www.endlesslycurious.com/reading-list/">Reading List</a></li>
<li class="page_item page-item-3820"><a href="http://www.endlesslycurious.com/tools/">Tools</a></li>
                </ul>
                
            
            </div>
        </div>
    </div><!-- end header -->
    
    <div id="main" class="clearfix">
    	<div class="contentwidth">
	<div class="column columnLarge">		
		

		<div class="post" id="post-2679">
				<h2>Finding duplicate files using Python</h2>
		<div class="postmeta">
			<p class="clearfix"><span>01.06.11</span><span>Posted in <a href="http://www.endlesslycurious.com/category/programming/" rel="category tag">Programming</a></span><span><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/#comments" title="Comment on Finding duplicate files using Python">19 Comments</a></span><span>Tags: <a href="http://www.endlesslycurious.com/tag/python/" rel="tag">Python</a></span></p>
		</div>
		<div class="postcontent">
			<p>I wrote this script to find and optionally delete duplicate files in a directory tree. &nbsp;The script uses&nbsp;<a href="http://en.wikipedia.org/wiki/MD5">MD5</a> hashes of each file’s content to detect duplicate files. This script is based on zalew’s <a href="http://stackoverflow.com/questions/748675/finding-duplicate-files-and-removing-them/748879#748879">answer</a> on stackoverflow. So far I have found this script sufficient for accurately finding and removing duplicate files in my photograph collection.</p>
<div><div id="highlighter_623558" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number1 index0 alt2">1</div><div class="line number2 index1 alt1">2</div><div class="line number3 index2 alt2">3</div><div class="line number4 index3 alt1">4</div><div class="line number5 index4 alt2">5</div><div class="line number6 index5 alt1">6</div><div class="line number7 index6 alt2">7</div><div class="line number8 index7 alt1">8</div><div class="line number9 index8 alt2">9</div><div class="line number10 index9 alt1">10</div><div class="line number11 index10 alt2">11</div><div class="line number12 index11 alt1">12</div><div class="line number13 index12 alt2">13</div><div class="line number14 index13 alt1">14</div><div class="line number15 index14 alt2">15</div><div class="line number16 index15 alt1">16</div><div class="line number17 index16 alt2">17</div><div class="line number18 index17 alt1">18</div><div class="line number19 index18 alt2">19</div><div class="line number20 index19 alt1">20</div><div class="line number21 index20 alt2">21</div><div class="line number22 index21 alt1">22</div><div class="line number23 index22 alt2">23</div><div class="line number24 index23 alt1">24</div><div class="line number25 index24 alt2">25</div><div class="line number26 index25 alt1">26</div><div class="line number27 index26 alt2">27</div><div class="line number28 index27 alt1">28</div><div class="line number29 index28 alt2">29</div><div class="line number30 index29 alt1">30</div><div class="line number31 index30 alt2">31</div><div class="line number32 index31 alt1">32</div><div class="line number33 index32 alt2">33</div><div class="line number34 index33 alt1">34</div><div class="line number35 index34 alt2">35</div><div class="line number36 index35 alt1">36</div><div class="line number37 index36 alt2">37</div><div class="line number38 index37 alt1">38</div><div class="line number39 index38 alt2">39</div><div class="line number40 index39 alt1">40</div><div class="line number41 index40 alt2">41</div><div class="line number42 index41 alt1">42</div><div class="line number43 index42 alt2">43</div><div class="line number44 index43 alt1">44</div><div class="line number45 index44 alt2">45</div><div class="line number46 index45 alt1">46</div></td><td class="code"><div class="container"><div class="line number1 index0 alt2"><code class="python comments">"""Find duplicate files inside a directory tree."""</code></div><div class="line number2 index1 alt1">&nbsp;</div><div class="line number3 index2 alt2"><code class="python keyword">from</code> <code class="python plain">os </code><code class="python keyword">import</code> <code class="python plain">walk, remove, stat</code></div><div class="line number4 index3 alt1"><code class="python keyword">from</code> <code class="python plain">os.path </code><code class="python keyword">import</code> <code class="python plain">join as joinpath</code></div><div class="line number5 index4 alt2"><code class="python keyword">from</code> <code class="python plain">md5 </code><code class="python keyword">import</code> <code class="python plain">md5</code></div><div class="line number6 index5 alt1">&nbsp;</div><div class="line number7 index6 alt2"><code class="python keyword">def</code> <code class="python plain">find_duplicates( rootdir ):</code></div><div class="line number8 index7 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments">"""Find duplicate files in directory tree."""</code></div><div class="line number9 index8 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">filesizes </code><code class="python keyword">=</code> <code class="python plain">{}</code></div><div class="line number10 index9 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># Build up dict with key as filesize and value is list of filenames.</code></div><div class="line number11 index10 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">path, dirs, files </code><code class="python keyword">in</code> <code class="python plain">walk( rootdir ):</code></div><div class="line number12 index11 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">filename </code><code class="python keyword">in</code> <code class="python plain">files:</code></div><div class="line number13 index12 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">filepath </code><code class="python keyword">=</code> <code class="python plain">joinpath( path, filename )</code></div><div class="line number14 index13 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">filesize </code><code class="python keyword">=</code> <code class="python plain">stat( filepath ).st_size</code></div><div class="line number15 index14 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">filesizes.setdefault( filesize, [] ).append( filepath )</code></div><div class="line number16 index15 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">unique </code><code class="python keyword">=</code> <code class="python functions">set</code><code class="python plain">()</code></div><div class="line number17 index16 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">duplicates </code><code class="python keyword">=</code> <code class="python plain">[]</code></div><div class="line number18 index17 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python comments"># We are only interested in lists with more than one entry.</code></div><div class="line number19 index18 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">files </code><code class="python keyword">in</code> <code class="python plain">[ flist </code><code class="python keyword">for</code> <code class="python plain">flist </code><code class="python keyword">in</code> <code class="python plain">filesizes.values() </code><code class="python keyword">if</code> <code class="python functions">len</code><code class="python plain">(flist)&gt;</code><code class="python value">1</code> <code class="python plain">]:</code></div><div class="line number20 index19 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">filepath </code><code class="python keyword">in</code> <code class="python plain">files:</code></div><div class="line number21 index20 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">with </code><code class="python functions">open</code><code class="python plain">( filepath ) as openfile:</code></div><div class="line number22 index21 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">filehash </code><code class="python keyword">=</code> <code class="python plain">md5( openfile.read() ).hexdigest()</code></div><div class="line number23 index22 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">if</code> <code class="python plain">filehash </code><code class="python keyword">not</code> <code class="python keyword">in</code> <code class="python plain">unique:</code></div><div class="line number24 index23 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">unique.add( filehash )</code></div><div class="line number25 index24 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">else</code><code class="python plain">:</code></div><div class="line number26 index25 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">duplicates.append( filepath )</code></div><div class="line number27 index26 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">return</code> <code class="python plain">duplicates</code></div><div class="line number28 index27 alt1">&nbsp;</div><div class="line number29 index28 alt2"><code class="python keyword">if</code> <code class="python plain">__name__ </code><code class="python keyword">=</code><code class="python keyword">=</code> <code class="python string">'__main__'</code><code class="python plain">:</code></div><div class="line number30 index29 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">from</code> <code class="python plain">argparse </code><code class="python keyword">import</code> <code class="python plain">ArgumentParser</code></div><div class="line number31 index30 alt2">&nbsp;</div><div class="line number32 index31 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">PARSER </code><code class="python keyword">=</code> <code class="python plain">ArgumentParser( description</code><code class="python keyword">=</code><code class="python string">'Finds duplicate files.'</code> <code class="python plain">)</code></div><div class="line number33 index32 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">PARSER.add_argument( </code><code class="python string">'root'</code><code class="python plain">, metavar</code><code class="python keyword">=</code><code class="python string">'R'</code><code class="python plain">, </code><code class="python functions">help</code><code class="python keyword">=</code><code class="python string">'Dir to search.'</code> <code class="python plain">)</code></div><div class="line number34 index33 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">PARSER.add_argument( </code><code class="python string">'-remove'</code><code class="python plain">, action</code><code class="python keyword">=</code><code class="python string">'store_true'</code><code class="python plain">,</code></div><div class="line number35 index34 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">help</code><code class="python keyword">=</code><code class="python string">'Delete duplicate files.'</code> <code class="python plain">)</code></div><div class="line number36 index35 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">ARGS </code><code class="python keyword">=</code> <code class="python plain">PARSER.parse_args()</code></div><div class="line number37 index36 alt2">&nbsp;</div><div class="line number38 index37 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">DUPS </code><code class="python keyword">=</code> <code class="python plain">find_duplicates( ARGS.root )</code></div><div class="line number39 index38 alt2">&nbsp;</div><div class="line number40 index39 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code> <code class="python string">'%d Duplicate files found.'</code> <code class="python keyword">%</code> <code class="python functions">len</code><code class="python plain">(DUPS)</code></div><div class="line number41 index40 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">for</code> <code class="python plain">f </code><code class="python keyword">in</code> <code class="python functions">sorted</code><code class="python plain">(DUPS):</code></div><div class="line number42 index41 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">if</code> <code class="python plain">ARGS.remove </code><code class="python keyword">=</code><code class="python keyword">=</code> <code class="python color1">True</code><code class="python plain">:</code></div><div class="line number43 index42 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">remove( f )</code></div><div class="line number44 index43 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code> <code class="python string">'\tDeleted '</code><code class="python keyword">+</code> <code class="python plain">f</code></div><div class="line number45 index44 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python keyword">else</code><code class="python plain">:</code></div><div class="line number46 index45 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python functions">print</code> <code class="python string">'\t'</code><code class="python keyword">+</code> <code class="python plain">f</code></div></div></td></tr></tbody></table></div></div>
<p>I discovered the <a href="http://docs.python.org/library/argparse">argparse</a> module (added in Python 2.7) in the standard library this week and it makes command line parameter handling nice and concise.</p>
<p>UPDATE: Changed uniques array into a set and added first pass using file sizes as performance improvement, allot faster now.</p>
<p>UPDATE: You can now find this script on github at <a href="https://github.com/endlesslycurious/Duplicate-Files" title="Duplicate Files on github">github.com/endlesslycurious/Duplicate-Files</a>.</p>
		</div>                   	
		
				
		
		<p>
			<small>
				Both comments and pings are currently closed.
							</small>
		</p>

                <div class="socialcontent">
                    <div class="socialbutton">
                        <!-- facebook like button -->
<iframe src="./Finding duplicate files using Python   Endlessly Curious_files/saved_resource.html" scrolling="no" frameborder="0" style="border: none; overflow: hidden; width: 125px; height: 30px; display: none !important;" allowtransparency="true"></iframe>
                        </div>               
                        <div class="socialbutton">
                        <!-- reddit button -->
<script type="text/javascript">reddit_title='Finding duplicate files using Python';</script>
<script type="text/javascript">reddit_url='http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/';</script>
<script type="text/javascript" src="http://www.redditstatic.com/button/button1.js"></script>
                        </div>         	
                        <div class="socialbutton">
                        <!-- twitter button -->
<iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" src="./Finding duplicate files using Python   Endlessly Curious_files/saved_resource.html" class="twitter-share-button twitter-tweet-button twitter-share-button twitter-count-horizontal" title="Twitter Tweet Button" data-twttr-rendered="true" style="width: 108px; height: 20px; display: none !important;"></iframe>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
                        </div>
                </div>
  
 
		</div>
			
		<div class="navigation clearfix">
			<div class="alignleft"><a href="http://www.endlesslycurious.com/2011/05/27/garr-lessons-from-bamboo/" rel="prev">Garr – Lessons from Bamboo</a></div>
			<div class="alignright"><a href="http://www.endlesslycurious.com/2011/06/10/python-2-7-1-goodness/" rel="next">Python 2.7.1 Goodness</a></div>
		</div>

	
<!-- You can start editing here. -->

	<h3 id="comments">19 Responses to “Finding duplicate files using Python”</h3>

	<div class="navigation clearfix">
		<div class="alignleft"></div>
		<div class="alignright"></div>
	</div>

	<ol class="commentlist">
			<li class="comment even thread-even depth-1" id="comment-1398">
				<div id="div-comment-1398" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/c9971be608728f0f2294865521daceba" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.dev-tricks.net/" rel="external nofollow" class="url">Julien Palard</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1398">
			01/06/2011 at 04:34</a>		</div>

		<p>You should use a set() instead of an array to store uniques checksums, you’ll have a ~constant time while seeking for a collision. Using an array, python does a linear search in the array, taking more and more time. (A little test shows that to store 10000 md5 in an array checking for collisions takes ~2s, in a set: ~0.1s).</p>
<p>Then shouldn’t you use a fastest ‘hash’ in a first pass to take apart very different files without opening them, like, file size ?</p>
<p>Finaly why reinvent the whell, just use fdupes <img src="./Finding duplicate files using Python   Endlessly Curious_files/icon_smile.gif" alt=":)" class="wp-smiley"> </p>
<p>Also before deleting you should binary compare the files, in case of md5 collision.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1399">
				<div id="div-comment-1399" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/1b58539751e7be3ed5399d0e952a7b40" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://sateeshkumarb.wordpress.com/" rel="external nofollow" class="url">Sateesh</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1399">
			01/06/2011 at 04:49</a>		</div>

		<p>The return value of hexdigest() is a string. So in the code ‘unique’ can be a set instead of list. Making ‘unique’ a set makes the intent more clear and also a ‘not in’ on a set would be faster</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1400">
				<div id="div-comment-1400" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/0263cded7f48e42a23cc519d8fdffb10" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://flowblok.id.au/" rel="external nofollow" class="url">Peter Ward</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1400">
			01/06/2011 at 04:53</a>		</div>

		<p>Why not:</p>
<p>…<br>
def find_duplicates( rootdir ):<br>
    “””Find duplicate files in directory tree.”””<br>
    unique = set()<br>
    for path, dirs, files in walk( rootdir ):<br>
        for filename in files:<br>
            filepath = joinpath(path, filename )<br>
            filehash = md5(open(filepath , ‘rb’).read()).digest()<br>
            if filehash not in unique:<br>
                unique.add(filehash)<br>
            else:<br>
                yield filepath<br>
…</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1402">
				<div id="div-comment-1402" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/a8da60916c206fd8d6aa15460d13e5eb" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Artur</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1402">
			01/06/2011 at 06:47</a>		</div>

		<p>Hm, but this doesn’t show which files are the same? I would rather use a dictionary that maps from md5 sum to filepaths:</p>
<p>from collections import defaultdict</p>
<p>def find_duplicates( rootdir ):<br>
    dup = defaultdict(list)<br>
    for path, dirs, files in walk( rootdir ):<br>
        for filename in files:<br>
            filepath = joinpath( path, filename )<br>
            try:<br>
                filehash = md5( file( filepath ).read() ).hexdigest()<br>
            except IOError:<br>
                continue<br>
            dup[filehash].append(filepath)<br>
    return [paths for paths in dup.values() if len(paths) &gt; 1]</p>
<p>The whole script can be also simulated in this simple shell action:</p>
<p>$ find . -type f -exec md5sum {} \; | uniq -d -w 32</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1403">
				<div id="div-comment-1403" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/d80d34e09ba28fe7a8cd59c1dc09f88f" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Jerry</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1403">
			01/06/2011 at 07:01</a>		</div>

		<p>If your files tend to be large (like video files), or file access is low (like files on a network share), you may want to do the duplicate detection in stages.  </p>
<p>It can be relatively expensive to read in a whole file and calculate an MD5 hash. It’s cheap to find out a file’s size, though, and files with different sizes cannot be duplicates of each other.  So if you start by creating a list of files along with their sizes, you can go back and only calculate the MD5 hash for the files that have identical lengths.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1404">
				<div id="div-comment-1404" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/cd8b5334a6788e0fc3b11814d090f552" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Marko Tasic</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1404">
			01/06/2011 at 07:40</a>		</div>

		<p>Nice article. You could use set instead of list to speedup lookups.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1405">
				<div id="div-comment-1405" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/9526779da056a47fb468095836dc0f73" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">xtian</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1405">
			01/06/2011 at 09:20</a>		</div>

		<p>If you’ve got a lot of files, the ‘filehash not in uniques’ check will get very expensive. This is because Python has to scan through the list one by one looking for filehash. So if you have N files, it will take O(N^2) time.</p>
<p>If you initialise unique with set() instead of [], the ‘not in’ check will be quick even with a lot of distinct files.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1406">
				<div id="div-comment-1406" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/247b22abbfc605098b654d28067b60a0" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.endlesslycurious.com/" rel="external nofollow" class="url">Daniel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1406">
			01/06/2011 at 10:17</a>		</div>

		<p>Thank you for all the comments! </p>
<p>RE: Set vs Array<br>
Regarding using a set instead of a array I was only running over a set of 5000 files so I didn’t notice the speed issue too much as my harddrive is SSD.  I’ll look into making the change to a set, as it sounds like a good optimisation given the performance numbers your giving me.</p>
<p>RE: Juian<br>
I did not know about fdupes, as I’m still relatively new to the Unix/Mac command line.  I inspected all the duplicate files manually before running the script again to remove them.</p>
<p>RE: Artur – dictionaries<br>
I did think about a dictionary and more in depth reporting of the location of duplicates vs the original file but I found that duplicates and the original tended to be in the same folder if my use case.  It could be a useful feature though.</p>
<p>RE: Jerry &amp; Julian – File sizes test<br>
I like the idea of a first pass using file sizes, that would be a good optimization.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1407">
				<div id="div-comment-1407" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/247b22abbfc605098b654d28067b60a0" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.endlesslycurious.com/" rel="external nofollow" class="url">Daniel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1407">
			01/06/2011 at 11:22</a>		</div>

		<p>I updated the post with a new version of find_duplicates which uses a first pass comparing file sizes and a second pass which compares hashes of files that are the same size  The new version is allot faster than the original, many thanks for all the advice!</p>
<p>It would be fairly simple to tweak the new  version to return a dictionary of duplicate files with the hash as the keys and a list of filenames as the values.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1408">
				<div id="div-comment-1408" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/4446f50fa72a1b8fe7fee62381757404" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://blog.liw.fi/" rel="external nofollow" class="url">Lars Wirzenius</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1408">
			01/06/2011 at 13:59</a>		</div>

		<p>I wrote a tool like this, too. It compares files byte-by-byte, which can be quite a lot of files than hashing the whole contents. See <a href="http://liw.fi/dupfiles/" rel="nofollow">http://liw.fi/dupfiles/</a> for my program. That page lists a few other tools too, the fastest of which seems to be the one called hardlink (<a href="http://jak-linux.org/projects/hardlink/" rel="nofollow">http://jak-linux.org/projects/hardlink/</a>).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1409">
				<div id="div-comment-1409" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/d2da98a484e78bcf7c369aa468610568" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://mcwalter.org/" rel="external nofollow" class="url">Finlay McWalter</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1409">
			01/06/2011 at 14:20</a>		</div>

		<p>Regarding the md5 hashes: it’s really not necessary to hash the whole file.  When I did a similar task I indexed on file size as Jerry did, then on an md5 has of the first 1kbytes, and only if there were matches on that did I do a full file hash. </p>
<p>Conversely, if one compares the md5(1k) hashes (and deliberately ignores file sizes) the program can find files where one is a truncated version of another.</p>
<p>Lastly, your program now stores its results only in a volatile storage, which means if the task is big (if you’re looking for duplicates among millions of files, for example) interrupting it or rerunning it means all the work must be done again. If you store the data instead in a persistent store like PyBSDDB then you can resume and rerun with little additional work.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1411">
				<div id="div-comment-1411" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/247b22abbfc605098b654d28067b60a0" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.endlesslycurious.com/" rel="external nofollow" class="url">Daniel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1411">
			01/06/2011 at 15:02</a>		</div>

		<p>RE: Lars<br>
I like your solution of replacing duplicates with links between the two distributions – very efficient!</p>
<p>RE: Finlay<br>
My script is naive in a few ways as I wrote it to deal with my photograph collection which had approximately 5500 files of which about 250 were duplicates.  So I felt that hashing the whole file was not too extreme a thing to do, although as you say it wouldn’t be a great idea for very large numbers of files unless you had a correspondingly large amount of RAM available.</p>
<p>Storing results between runs I in database like BerkeleyDB or SQLite would be very useful for very large filesets.  For the fileset size I am dealing with I tend to use the csv module for outputting debug information as I can use Excel to inspect it.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1413">
				<div id="div-comment-1413" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/482f6832b98eccb86e2a5dc4de8aad91" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Dag</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1413">
			02/06/2011 at 03:18</a>		</div>

		<p><em>Repost with working link:</em></p>
<p>I use <a href="http://code.google.com/p/fdupes/" rel="nofollow">fdupes</a> for this, it’s packaged in at least Ubuntu (sudo apt-get install fdupes).</p>
<p>For photos there’s also <a href="http://www.jhnc.org/findimagedupes/" rel="nofollow">findimagedupes</a> (sudo apt-get install findimagedupes) which even finds visually similar images.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1415">
				<div id="div-comment-1415" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/80063f504840e6f02575cf4124d25361" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Masaru</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1415">
			03/06/2011 at 03:16</a>		</div>

		<p>Why nobody is wondering, that the “open” file handles won’t be closed?</p>
<p>Open file objects should be closed: always!</p>
<p>The use of the “with” statement (Python 2.6+) may simplify this really good.</p>
<p>with file(filepath) as open_file:<br>
    filehash = md5(open_file.read()).hexdigest()</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-1417">
				<div id="div-comment-1417" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/247b22abbfc605098b654d28067b60a0" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.endlesslycurious.com/" rel="external nofollow" class="url">Daniel</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1417">
			03/06/2011 at 18:28</a>		</div>

		<p>RE: Dag<br>
I seem to suck at finding existing tools for these things, where is the best place to look for open source tools other than Google?</p>
<p>RE: Masaru<br>
Good catch, I’ve updated the script.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-1418">
				<div id="div-comment-1418" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/4446f50fa72a1b8fe7fee62381757404" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://blog.liw.fi/" rel="external nofollow" class="url">Lars Wirzenius</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1418">
			03/06/2011 at 22:34</a>		</div>

		<p>Daniel, the two places I usually look at are Debian (30 kilopackages and growing), since that makes it easy for me to install if it’s there, and <a href="http://freshmeat.net/" rel="nofollow">http://freshmeat.net/</a> (who would like to list everything free).</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="pingback even thread-even depth-1" id="comment-1704">
				<div id="div-comment-1704" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href="http://www.panisch.com/2012/03/how-to-delete-duplicate-files-using-python/" rel="external nofollow" class="url">How to delete duplicate files using python | PANISCH</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-1704">
			12/03/2012 at 23:36</a>		</div>

		<p>[…] enhance convenience I extended the original script from Daniel Brown by adding a graphical user interface using Tkinter, which is integrated in […]</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-2366">
				<div id="div-comment-2366" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/ba0d6c49aa3884405d3d4e7f5ad1c5b8" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn">Thanks</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-2366">
			21/05/2013 at 00:29</a>		</div>

		<p>Hey Daniel, thanks for the program!  I’m new to python and your program is a great example and help to me for study as well as utility.</p>

		
				</div>
		</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1" id="comment-2410">
				<div id="div-comment-2410" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Finding duplicate files using Python   Endlessly Curious_files/40c6b748f667d49e3cbe3c89adb26187" class="avatar avatar-32 photo" height="32" width="32">			<cite class="fn"><a href="http://www.mindgems.com/" rel="external nofollow" class="url">Jack Dorsey</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/comment-page-1/#comment-2410">
			23/06/2013 at 23:20</a>		</div>

		<p>I am using this tool to <a href="http://www.mindgems.com/products/Fast-Duplicate-File-Finder/Fast-Duplicate-File-Finder-About.htm" rel="nofollow">Find Similar Documents</a>.<br>
Pretty impressive results – give it a try,</p>

		
				</div>
		</li><!-- #comment-## -->
	</ol>

	<div class="navigation clearfix">
		<div class="alignleft"></div>
		<div class="alignright"></div>
	</div>
 


	
</div><!-- end column large-->

<div class="column columnSmall sidebar last">

	<div class="column columnSmall">
		<div id="search-2" class="widget widget_search"><h3 class="widgettitle">Search this blog</h3><form name="searchform" id="searchform" class="clearfix" method="get" action="http://www.endlesslycurious.com/">
	<input class="text" type="text" name="s" id="s" value="">
	<input id="searchsubmit" type="image" src="./Finding duplicate files using Python   Endlessly Curious_files/icon_search.jpg" value="">
</form></div>		<div id="recent-posts-3" class="widget widget_recent_entries">		<h3 class="widgettitle">Recent Posts</h3>		<ul>
					<li>
				<a href="http://www.endlesslycurious.com/2014/12/27/december-snapshots/">December Snapshots</a>
						</li>
					<li>
				<a href="http://www.endlesslycurious.com/2014/12/05/switching-back-to-fuji-from-nikon/">Switching back to Fuji from Nikon</a>
						</li>
					<li>
				<a href="http://www.endlesslycurious.com/2014/11/29/november-snapshot/">November Snapshot</a>
						</li>
					<li>
				<a href="http://www.endlesslycurious.com/2014/11/13/json-pretty-printer-in-python/">JSON Pretty Printer in Python</a>
						</li>
					<li>
				<a href="http://www.endlesslycurious.com/2014/10/30/october-snapshots/">October Snapshots</a>
						</li>
				</ul>
		</div><div id="get-recent-comments" class="widget widget_get_recent_comments"><h3 class="widgettitle">Recent Comments</h3><div id="get_recent_comments_wrap"><ul>	<li>Liza on <a href="http://www.endlesslycurious.com/2014/01/19/copenhagen/#comment-39420" title="Copenhagen, 19/01/2014">Copenhagen</a>.</li>
	<li>Yevgene on <a href="http://www.endlesslycurious.com/2012/06/13/scraping-pdf-with-python/#comment-32426" title="Scraping PDF with Python, 13/06/2012">Scraping PDF with Python</a>.</li>
	<li>Gordon Morehouse on <a href="http://www.endlesslycurious.com/2011/05/11/extracting-image-exif-data-with-python/#comment-11885" title="Extracting image EXIF data with Python, 11/05/2011">Extracting image EXIF data with Python</a>.</li>
	<li>Colin Wren on <a href="http://www.endlesslycurious.com/2012/06/13/scraping-pdf-with-python/#comment-8135" title="Scraping PDF with Python, 13/06/2012">Scraping PDF with Python</a>.</li>
	<li>Yomatt on <a href="http://www.endlesslycurious.com/2013/08/12/five-years-of-endlessly-curious/#comment-7786" title="Five years of Endlessly Curious!, 12/08/2013">Five years of Endlessly Curious!</a>.</li>
</ul></div></div><div id="text-2" class="widget widget_text"><h3 class="widgettitle">About Daniel Brown</h3>			<div class="textwidget"><img title="Daniel Brown" src="./Finding duplicate files using Python   Endlessly Curious_files/20120424-199x199.jpg"><br>
Daniel Brown is a Scotsman working as a game developer in Stockholm, Sweden.   These are his words.

<br><br>
<div class="about-social-links">
<a href="http://www.facebook.com/EndlesslyCurious"><img src="./Finding duplicate files using Python   Endlessly Curious_files/facebook-48.png" title="Friend me on FaceBook." class="si-facebook"></a>
<a href="https://github.com/endlesslycurious"><img src="./Finding duplicate files using Python   Endlessly Curious_files/github-48.png" title="My Code on Github." class="si-github"></a>
<a href="http://instagram.com/endlessly_curious"><img src="http://www.endlesslycurious.com/wp-content/simple-icons/instagram-48.png" title="My photographs on Instagram." class="si-instagram" style="display: none !important;"></a>
<a href="http://ca.linkedin.com/in/danielpeterbrown"><img src="http://www.endlesslycurious.com/wp-content/simple-icons/linkedin-48.png" title="Network on Linked In" class="si-linkedin" style="display: none !important;"></a>
<a href="http://www.twitter.com/dpbrown"><img src="./Finding duplicate files using Python   Endlessly Curious_files/twitter-48.png" title="Follow me on Twitter." class="si-twitter"></a>
</div></div>
		</div><div id="categories-3" class="widget widget_categories"><h3 class="widgettitle">Post Categories</h3>		<ul>
	<li class="cat-item cat-item-167"><a href="http://www.endlesslycurious.com/category/books/" title="Books I&#39;ve read.">Books</a>
</li>
	<li class="cat-item cat-item-1"><a href="http://www.endlesslycurious.com/category/miscellaneous/">Miscellaneous</a>
</li>
	<li class="cat-item cat-item-142"><a href="http://www.endlesslycurious.com/category/photography/">Photography</a>
</li>
	<li class="cat-item cat-item-118"><a href="http://www.endlesslycurious.com/category/productivity-2/">Productivity</a>
</li>
	<li class="cat-item cat-item-7"><a href="http://www.endlesslycurious.com/category/programming/">Programming</a>
</li>
	<li class="cat-item cat-item-146"><a href="http://www.endlesslycurious.com/category/travel-2/">Travel</a>
</li>
		</ul>
</div><div id="archives-4" class="widget widget_archive"><h3 class="widgettitle">Post Archives</h3>		<ul>
	<li><a href="http://www.endlesslycurious.com/2014/12/">December 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/11/">November 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/10/">October 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/09/">September 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/08/">August 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/07/">July 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/06/">June 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/05/">May 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/04/">April 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/03/">March 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/02/">February 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2014/01/">January 2014</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/12/">December 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/11/">November 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/10/">October 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/09/">September 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/08/">August 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/07/">July 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/06/">June 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/05/">May 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/04/">April 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/03/">March 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/02/">February 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2013/01/">January 2013</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/12/">December 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/11/">November 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/10/">October 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/09/">September 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/08/">August 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/07/">July 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/06/">June 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/05/">May 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/04/">April 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/03/">March 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/02/">February 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2012/01/">January 2012</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/12/">December 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/11/">November 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/10/">October 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/09/">September 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/08/">August 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/06/">June 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/05/">May 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/04/">April 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/03/">March 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/02/">February 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2011/01/">January 2011</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/12/">December 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/11/">November 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/10/">October 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/09/">September 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/08/">August 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/07/">July 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/06/">June 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/05/">May 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/04/">April 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/03/">March 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/02/">February 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2010/01/">January 2010</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/12/">December 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/11/">November 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/10/">October 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/09/">September 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/08/">August 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/07/">July 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/06/">June 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/05/">May 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/04/">April 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/03/">March 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/02/">February 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2009/01/">January 2009</a></li>
	<li><a href="http://www.endlesslycurious.com/2008/12/">December 2008</a></li>
	<li><a href="http://www.endlesslycurious.com/2008/11/">November 2008</a></li>
	<li><a href="http://www.endlesslycurious.com/2008/10/">October 2008</a></li>
	<li><a href="http://www.endlesslycurious.com/2008/09/">September 2008</a></li>
	<li><a href="http://www.endlesslycurious.com/2008/08/">August 2008</a></li>
		</ul>
</div>	</div>
</div>


        
        </div>
    </div><!-- end main -->
    

	<div id="footer-top" class="clearfix">
    
    	</div><!-- end footer-top -->
    
    
    
    <div id="footer-bottom" class="clearfix">
		<div class="contentwidth"><p class="right"><small><a href="http://www.endlesslycurious.com/2011/06/01/finding-duplicate-files-using-python/#top">Back to the Top</a></small></p>
			<p><small>Copyright © Daniel Brown 2008-2012.  <b>Disclaimer:</b> The views expressed on this blog are my own and not that of my employer.</small></p>    	</div>
    </div><!-- end footer-bottom -->


</div><!-- end outer -->

<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/jquery(1).js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/hoverIntent.js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/superfish.js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/supersubs.js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/general.js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/shCore.js"></script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/shBrushPython.js"></script>
<script type="text/javascript">
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "http://www.endlesslycurious.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.getElementsByTagName("head")[0].insertBefore( corecss, document.getElementById("syntaxhighlighteranchor") );
		var themecssurl = "http://www.endlesslycurious.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		//document.getElementById("syntaxhighlighteranchor").appendChild(themecss);
		document.getElementsByTagName("head")[0].insertBefore( themecss, document.getElementById("syntaxhighlighteranchor") );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();
</script>
<script type="text/javascript" src="./Finding duplicate files using Python   Endlessly Curious_files/wp-gallery-custom-links.js"></script>
<!--stats_footer_test--><script src="http://stats.wordpress.com/e-201513.js" type="text/javascript"></script>
<script type="text/javascript">
st_go({blog:'5488532',v:'ext',post:'2679'});
var load_cmc = function(){linktracker_init(5488532,2679,2);};
if ( typeof addLoadEvent != 'undefined' ) addLoadEvent(load_cmc);
else load_cmc();
</script>




<!-- Dynamic page generated in 0.505 seconds. -->
<!-- Cached page generated by WP-Super-Cache on 2015-03-24 11:08:56 -->

<!-- Compression = gzip --></body></html>